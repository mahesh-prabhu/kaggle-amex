{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2c62f1",
   "metadata": {},
   "source": [
    "Inspired by https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a61ab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "# GPU LIBRARIES, are these useful? Do they result in any meaningful speedup? I have replaced cupy with np and cudf with pd in the below code.\n",
    "#import cupy, cudf \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbcb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "MAIN_PATH = \"/home/mahesh/Desktop/ML/kaggle/amex/\"\n",
    "\n",
    "# Data\n",
    "PATH_TO_DATA                = MAIN_PATH + \"data/\"\n",
    "PATH_TO_PROCESSED_DATA      = PATH_TO_DATA + \"processed/\"\n",
    "FILENAME_TRAIN_DATA_CSV     = PATH_TO_DATA + \"orig/train_data.csv\"\n",
    "FILENAME_TRAIN_LABELS_CSV   = PATH_TO_DATA + \"orig/train_labels.csv\"\n",
    "FILENAME_CID_MAP            = PATH_TO_PROCESSED_DATA + \"cid_map.csv\"\n",
    "FILENAME_TRAIN_DATA_FEATHER = PATH_TO_PROCESSED_DATA + \"train_data.f\"\n",
    "\n",
    "# Models\n",
    "PATH_TO_MODEL   = MAIN_PATH + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-12 18:12:17.580682\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "#train_df = pd.read_feather(FILENAME_TRAIN_DATA_FEATHER)\n",
    "train_df = pd.read_csv(FILENAME_TRAIN_DATA_CSV)\n",
    "T_COLS = train_df.columns\n",
    "print(datetime.datetime.now())\n",
    "train_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f1a5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =  pd.read_csv(FILENAME_TRAIN_LABELS_CSV)\n",
    "targets['customer_ID'] = targets['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92673b",
   "metadata": {},
   "source": [
    "#### Split train data into folds\n",
    "\n",
    "For now we will just have 10 folds of non-stratified data.   \n",
    "***todo:*** *Need to stratify the folds to make sure they have similar ditribution of data. One way to do this is to use the distribution of output labels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edec7227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = train_df[\"customer_ID\"]\n",
    "customers = customers.drop_duplicates().sort_index().values.flatten()\n",
    "len(customers)\n",
    "type(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1897a2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will split train data into 10 separate folds.\n",
      "There will be 45891 customers in each fold (except the last fold).\n",
      "Below are number of rows in each fold:\n",
      "[553403, 552855, 554025, 554330, 552004, 552378, 552822, 553151, 553493, 552990]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rows(customers, train, NUM_FOLDS = 10, verbose = ''):\n",
    "    chunk = len(customers)//NUM_FOLDS\n",
    "    if verbose != '':\n",
    "        print(f'We will split {verbose} data into {NUM_FOLDS} separate folds.')\n",
    "        print(f'There will be {chunk} customers in each fold (except the last fold).')\n",
    "        print('Below are number of rows in each fold:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_FOLDS):\n",
    "        if k==NUM_FOLDS-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': print( rows )\n",
    "    return rows\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "rows = get_rows(customers, train_df, NUM_FOLDS = NUM_FOLDS, verbose = 'train')\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377fa8",
   "metadata": {},
   "source": [
    "#### Feature Engineering:\n",
    "\n",
    "    1. Handle NaNs \n",
    "    2. Make each customer have 13 months(?) of data, add zeroed out months? \n",
    "    3. Save processed data to disk? \n",
    "    4. Train/Validation split?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c76640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    #train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    train['customer_ID'] = train['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    train.S_2 = pd.to_datetime( train.S_2 )\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(1).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "            \n",
    "    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "    if PAD_CUSTOMER_TO_13_ROWS:\n",
    "        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "        more = np.array([],dtype='int64') \n",
    "        for j in range(1,13):\n",
    "            i = tmp.loc[tmp==j].index.values\n",
    "            more = np.concatenate([more,np.repeat(i,13-j)])\n",
    "        df = train.iloc[:len(more)].copy().fillna(0)\n",
    "        df = df * 0 - 1 #pad numerical columns with -1\n",
    "        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "        df['customer_ID'] = more\n",
    "        train = pd.concat([train,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    train = train[COLS]\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb0b51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_File_1 has 45891 customers and shape (596583, 190)\n",
      "Train_File_2 has 45891 customers and shape (596583, 190)\n",
      "Train_File_3 has 45891 customers and shape (596583, 190)\n",
      "Train_File_4 has 45891 customers and shape (596583, 190)\n",
      "Train_File_5 has 45891 customers and shape (596583, 190)\n",
      "Train_File_6 has 45891 customers and shape (596583, 190)\n",
      "Train_File_7 has 45891 customers and shape (596583, 190)\n",
      "Train_File_8 has 45891 customers and shape (596583, 190)\n",
      "Train_File_9 has 45891 customers and shape (596583, 190)\n",
      "Train_File_10 has 45894 customers and shape (596622, 190)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE PROCESSED TRAIN FOLDS AND SAVE TO DISK        \n",
    "print(datetime.datetime.now())\n",
    "for k in range(NUM_FOLDS):\n",
    "\n",
    "    # READ CHUNK OF TRAIN CSV FILE\n",
    "    skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "    train = pd.read_csv(FILENAME_TRAIN_DATA_CSV, nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "    # FEATURE ENGINEER DATAFRAME\n",
    "    train = feature_engineer(train, targets = targets)\n",
    "\n",
    "    # SAVE FILES\n",
    "    print(f'Train_File_{k+1} has {train.customer_ID.nunique()} customers and shape',train.shape)\n",
    "    tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n",
    "    #if not os.path.exists(PATH_TO_PROCESSED_DATA): os.makedirs(PATH_TO_DATA)\n",
    "    tar.to_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k+1}.pqt',index=False)\n",
    "    data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n",
    "    np.save(f'{PATH_TO_PROCESSED_DATA}data_{k+1}',data.astype('float32'))\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del train, tar, data\n",
    "    gc.collect()\n",
    "del targets\n",
    "gc.collect()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b6a51",
   "metadata": {},
   "source": [
    "#### Model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13f7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,188))\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10,4)\n",
    "        embeddings.append( emb(inp[:,:,k]) )\n",
    "    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed804f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LEARNING SCHEUDLE\n",
    "def lrfn(epoch):\n",
    "    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n",
    "    return lr[epoch]\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57ad835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPETITION METRIC FROM Konstantin Yakovlev\n",
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56a760",
   "metadata": {},
   "source": [
    "#### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4c4c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1 with valid files [1, 2]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 23s - loss: 0.2378 - val_loss: 0.2373 - lr: 0.0010 - 23s/epoch - 32ms/step\n",
      "Epoch 2/8\n",
      "718/718 - 21s - loss: 0.2268 - val_loss: 0.2288 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 3/8\n",
      "718/718 - 21s - loss: 0.2231 - val_loss: 0.2262 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 4/8\n",
      "718/718 - 21s - loss: 0.2207 - val_loss: 0.2263 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 5/8\n",
      "718/718 - 21s - loss: 0.2184 - val_loss: 0.2281 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 6/8\n",
      "718/718 - 21s - loss: 0.2120 - val_loss: 0.2233 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 7/8\n",
      "718/718 - 21s - loss: 0.2107 - val_loss: 0.2231 - lr: 1.0000e-04 - 21s/epoch - 30ms/step\n",
      "Epoch 8/8\n",
      "718/718 - 21s - loss: 0.2094 - val_loss: 0.2231 - lr: 1.0000e-05 - 21s/epoch - 29ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 2s - 2s/epoch - 13ms/step\n",
      "\n",
      "Fold 1 CV= 0.786837936725747\n",
      "\n",
      "#########################\n",
      "### Fold 2 with valid files [3, 4]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 23s - loss: 0.2390 - val_loss: 0.2355 - lr: 0.0010 - 23s/epoch - 33ms/step\n",
      "Epoch 2/8\n",
      "718/718 - 21s - loss: 0.2270 - val_loss: 0.2296 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 3/8\n",
      "718/718 - 21s - loss: 0.2238 - val_loss: 0.2272 - lr: 0.0010 - 21s/epoch - 30ms/step\n",
      "Epoch 4/8\n",
      "718/718 - 22s - loss: 0.2215 - val_loss: 0.2308 - lr: 0.0010 - 22s/epoch - 31ms/step\n",
      "Epoch 5/8\n",
      "718/718 - 22s - loss: 0.2193 - val_loss: 0.2283 - lr: 0.0010 - 22s/epoch - 31ms/step\n",
      "Epoch 6/8\n",
      "718/718 - 21s - loss: 0.2132 - val_loss: 0.2231 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 7/8\n",
      "718/718 - 21s - loss: 0.2119 - val_loss: 0.2234 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 8/8\n",
      "718/718 - 21s - loss: 0.2107 - val_loss: 0.2229 - lr: 1.0000e-05 - 21s/epoch - 29ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 2s - 2s/epoch - 12ms/step\n",
      "\n",
      "Fold 2 CV= 0.7832782321346827\n",
      "\n",
      "#########################\n",
      "### Fold 3 with valid files [5, 6]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 23s - loss: 0.2376 - val_loss: 0.2289 - lr: 0.0010 - 23s/epoch - 32ms/step\n",
      "Epoch 2/8\n",
      "718/718 - 21s - loss: 0.2263 - val_loss: 0.2282 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 3/8\n",
      "718/718 - 21s - loss: 0.2235 - val_loss: 0.2250 - lr: 0.0010 - 21s/epoch - 30ms/step\n",
      "Epoch 4/8\n",
      "718/718 - 21s - loss: 0.2210 - val_loss: 0.2226 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 5/8\n",
      "718/718 - 21s - loss: 0.2188 - val_loss: 0.2237 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 6/8\n",
      "718/718 - 21s - loss: 0.2126 - val_loss: 0.2214 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 7/8\n",
      "718/718 - 21s - loss: 0.2112 - val_loss: 0.2216 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 8/8\n",
      "718/718 - 21s - loss: 0.2100 - val_loss: 0.2212 - lr: 1.0000e-05 - 21s/epoch - 29ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 2s - 2s/epoch - 12ms/step\n",
      "\n",
      "Fold 3 CV= 0.7864661949349292\n",
      "\n",
      "#########################\n",
      "### Fold 4 with valid files [7, 8]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 23s - loss: 0.2396 - val_loss: 0.2414 - lr: 0.0010 - 23s/epoch - 32ms/step\n",
      "Epoch 2/8\n",
      "718/718 - 21s - loss: 0.2279 - val_loss: 0.2278 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 3/8\n",
      "718/718 - 21s - loss: 0.2241 - val_loss: 0.2245 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 4/8\n",
      "718/718 - 21s - loss: 0.2221 - val_loss: 0.2253 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 5/8\n",
      "718/718 - 21s - loss: 0.2199 - val_loss: 0.2233 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 6/8\n",
      "718/718 - 21s - loss: 0.2138 - val_loss: 0.2191 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 7/8\n",
      "718/718 - 21s - loss: 0.2125 - val_loss: 0.2195 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 8/8\n",
      "718/718 - 21s - loss: 0.2112 - val_loss: 0.2190 - lr: 1.0000e-05 - 21s/epoch - 29ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 2s - 2s/epoch - 12ms/step\n",
      "\n",
      "Fold 4 CV= 0.7885337571261182\n",
      "\n",
      "#########################\n",
      "### Fold 5 with valid files [9, 10]\n",
      "### Training data shapes (367128, 13, 188) (367128,)\n",
      "### Validation data shapes (91785, 13, 188) (91785,)\n",
      "#########################\n",
      "Epoch 1/8\n",
      "718/718 - 23s - loss: 0.2382 - val_loss: 0.2265 - lr: 0.0010 - 23s/epoch - 32ms/step\n",
      "Epoch 2/8\n",
      "718/718 - 21s - loss: 0.2274 - val_loss: 0.2234 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 3/8\n",
      "718/718 - 21s - loss: 0.2240 - val_loss: 0.2241 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 4/8\n",
      "718/718 - 21s - loss: 0.2219 - val_loss: 0.2278 - lr: 0.0010 - 21s/epoch - 29ms/step\n",
      "Epoch 5/8\n",
      "718/718 - 21s - loss: 0.2197 - val_loss: 0.2225 - lr: 0.0010 - 21s/epoch - 30ms/step\n",
      "Epoch 6/8\n",
      "718/718 - 21s - loss: 0.2133 - val_loss: 0.2189 - lr: 1.0000e-04 - 21s/epoch - 30ms/step\n",
      "Epoch 7/8\n",
      "718/718 - 21s - loss: 0.2117 - val_loss: 0.2189 - lr: 1.0000e-04 - 21s/epoch - 29ms/step\n",
      "Epoch 8/8\n",
      "718/718 - 21s - loss: 0.2104 - val_loss: 0.2187 - lr: 1.0000e-05 - 21s/epoch - 29ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 2s - 2s/epoch - 12ms/step\n",
      "\n",
      "Fold 5 CV= 0.7895364408550763\n",
      "\n",
      "#########################\n",
      "Overall CV = 0.7870073847022666\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # SAVE TRUE AND OOF\n",
    "    true = np.array([])\n",
    "    oof = np.array([])\n",
    "    VERBOSE = 2 # use 1 for interactive \n",
    "\n",
    "    for fold in range(5):\n",
    "\n",
    "        # INDICES OF TRAIN AND VALID FOLDS\n",
    "        valid_idx = [2*fold+1, 2*fold+2]\n",
    "        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n",
    "\n",
    "        print('#'*25)\n",
    "        print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "        # READ TRAIN DATA FROM DISK\n",
    "        X_train = []; y_train = []\n",
    "        for k in train_idx:\n",
    "            X_train.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_train.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_train = np.concatenate(X_train,axis=0)\n",
    "        y_train = pd.concat(y_train).target.values\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "\n",
    "        # READ VALID DATA FROM DISK\n",
    "        X_valid = []; y_valid = []\n",
    "        for k in valid_idx:\n",
    "            X_valid.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_valid.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_valid = np.concatenate(X_valid,axis=0)\n",
    "        y_valid = pd.concat(y_valid).target.values\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        h = model.fit(X_train,y_train, \n",
    "                      validation_data = (X_valid,y_valid),\n",
    "                      batch_size=512, epochs=8, verbose=VERBOSE,\n",
    "                      callbacks = [LR])\n",
    "        #if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n",
    "        model.save_weights(f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5')\n",
    "\n",
    "        # INFER VALID DATA\n",
    "        print('Inferring validation data...')\n",
    "        p = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n",
    "\n",
    "        print()\n",
    "        print(f'Fold {fold+1} CV=', amex_metric_mod(y_valid, p) )\n",
    "        print()\n",
    "        true = np.concatenate([true, y_valid])\n",
    "        oof = np.concatenate([oof, p])\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_train, y_train, X_valid, y_valid, p\n",
    "        gc.collect()\n",
    "\n",
    "    # PRINT OVERALL RESULTS\n",
    "    print('#'*25)\n",
    "    print(f'Overall CV =', amex_metric_mod(true, oof) )\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45098d",
   "metadata": {},
   "source": [
    "#### Model evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dd0dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
