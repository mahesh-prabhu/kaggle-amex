{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a61ab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "# GPU LIBRARIES, are these useful? Do they result in any meaningful speedup? I have replaced cupy with np and cudf with pd in the below code.\n",
    "#import cupy, cudf \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbcb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "MAIN_PATH = \"/home/mahesh/Desktop/ML/kaggle/amex/\"\n",
    "\n",
    "# Data\n",
    "PATH_TO_DATA                = MAIN_PATH + \"data/\"\n",
    "PATH_TO_PROCESSED2_DATA     = PATH_TO_DATA + \"processed2/\"\n",
    "#FILENAME_TRAIN_DATA_CSV     = PATH_TO_DATA + \"orig/train_data.csv\"\n",
    "FILENAME_TRAIN_LABELS_CSV   = PATH_TO_DATA + \"orig/train_labels.csv\"\n",
    "#FILENAME_TRAIN_DATA_FEATHER = PATH_TO_DATA + \"orig/train_data.f\"     \n",
    "\n",
    "# Processed data\n",
    "#FILENAME_CID_MAP                      = PATH_TO_PROCESSED_DATA + \"cid_map.csv\"\n",
    "FILENAME_TRAIN_PROCESSED_DATA_FEATHER = PATH_TO_PROCESSED2_DATA + \"train_data.f\"\n",
    "\n",
    "# Models\n",
    "PATH_TO_MODEL   = MAIN_PATH + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ca985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-30 18:57:55.338327\n",
      "2022-07-30 18:57:57.337160\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531451 entries, 0 to 5531450\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: category(11), datetime64[ns](1), float32(176), int32(1), int64(1)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "# Read from Feather\n",
    "train_df = pd.read_feather(FILENAME_TRAIN_PROCESSED_DATA_FEATHER)\n",
    "# Read from CSV\n",
    "#train_df = pd.read_csv(FILENAME_TRAIN_DATA_CSV)\n",
    "print(datetime.datetime.now())\n",
    "train_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1a5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets =  pd.read_csv(FILENAME_TRAIN_LABELS_CSV)\n",
    "#targets['customer_ID'] = targets['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662d49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features\n",
    "T_COLS = set(train_df.columns)\n",
    "# Features to be dropped that are irrelevant signals\n",
    "id_time_cols = set(['customer_ID', 'S_2'])\n",
    "# Categorical features\n",
    "cat_cols = set(['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68'])\n",
    "# Numerical features\n",
    "num_cols = T_COLS - id_time_cols - cat_cols\n",
    "\n",
    "#print(len(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15144b4",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "    1. Read the low-memory representation DF from Feather Format.  \n",
    "    2. Do a stratified split of data into train+val and test data. Which feature to do the stratified split on?  \n",
    "    3. Create feature engineering pipeline.  \n",
    "    4. Use decision tree to create a base model. \n",
    "    5. Idenitify important features, drop unimportant features, drop features with large number of NaNs?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84377fa8",
   "metadata": {},
   "source": [
    "#### Feature Engineering pipeline:\n",
    "\n",
    "    1. Make each customer have 13 months(?) of data, add zeroed out months?  \n",
    "    2. Split features into numerical and categorical.  \n",
    "    3. Numerical features:  \n",
    "        1. Handle NaNs : Fill with Median, Mean?\n",
    "        2. Feature scaling (only useful for NN models?) : Numerical data should be standardized to aid easier training.  \n",
    "    4. Categorical data :  \n",
    "        1. Handle NaNs : Fill with special value.  \n",
    "        2. One hot encoding ? Embeddings?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "106320b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self._attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return X[self._attribute_names].values\n",
    "\n",
    "class DataFrameCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self._attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return X[self._attribute_names].values    \n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, create_t, debug = False):\n",
    "        self._col_transformers = []\n",
    "        self._create_t = create_t\n",
    "        self._debug = debug\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        if self._debug:\n",
    "            print(datetime.datetime.now())\n",
    "        for col in range(X.shape[1]):\n",
    "            col_data_npa = (X[:,col]).reshape(-1,1)\n",
    "            t = self._create_t()\n",
    "            self._col_transformers.append(t.fit(col_data_npa))\n",
    "    \n",
    "        if self._debug:\n",
    "            print(datetime.datetime.now())\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        ctl = []\n",
    "        if self._debug:\n",
    "            print(datetime.datetime.now())\n",
    "            \n",
    "        for col in range(X.shape[1]):\n",
    "            col_data_npa = (X[:,col]).reshape(-1,1)\n",
    "            ctl.append(self._col_transformers.transform(col_data_npa))\n",
    "\n",
    "        res = np.concatenate(ctl,axis=1)\n",
    "        \n",
    "        if self._debug:\n",
    "            print(datetime.datetime.now())\n",
    "\n",
    "        return res\n",
    "\n",
    "    \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def apply_transformer(X_df, cols, create_t, debug = False):\n",
    "    col_transformers = []\n",
    "    ctl = []\n",
    "    if debug:\n",
    "        print(datetime.datetime.now())\n",
    "    for c in cols:\n",
    "        col_data_npa = X_df[[c]].values\n",
    "        #t = OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1)\n",
    "        t = create_t()\n",
    "        col_transformers.append(t)\n",
    "        col_data_trans_npa = t.fit_transform(col_data_npa)\n",
    "        ctl.append(cat_data_trans_npa)\n",
    "    \n",
    "    if debug:\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "    cols_transformed_npa = np.concatenate(ctl,axis=1)\n",
    "    \n",
    "    if debug:\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "    return (cols_transformed_npa, col_transformers)\n",
    "\n",
    "def create_simple_imputer_strategy_median():\n",
    "    return SimpleImputer(strategy=\"median\")\n",
    "\n",
    "\n",
    "def create_ordinal_encoder():\n",
    "    return OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1)\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_cols)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "    \n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('selector', DataFrameSelector(cat_cols)),\n",
    "#     ('encoder', OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1))\n",
    "# ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_cols)),\n",
    "    ('encoder', CustomTransformer(create_ordinal_encoder, debug = True))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ae6998a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 16:59:51.403635\n",
      "2022-08-01 16:59:55.818751\n",
      "2022-08-01 16:59:55.878931\n"
     ]
    }
   ],
   "source": [
    "#cat_pipeline.fit_transform(train_df)\n",
    "\n",
    "t = apply_transformer(train_df, cat_cols, create_ordinal_encoder, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ff39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_npa = train_df[(list(num_cols))[0:10]].values\n",
    "#num_data_npa = train_df[num_cols].values\n",
    "imputer = SimpleImputer(strategy=\"median\",copy = False)\n",
    "#print(num_data_npa[1])\n",
    "t = imputer.fit_transform(num_data_npa)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90e67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "le = OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1)\n",
    "#print(cat_data_npa[0:9])\n",
    "#le.fit_transform(cat_data_npa[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6519de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('categorical_transformer_0', OrdinalEncoder(dtype=<class 'numpy.int8'>, encoded_missing_value=-1), slice(0, 1, None)), ('categorical_transformer_1', OrdinalEncoder(dtype=<class 'numpy.int8'>, encoded_missing_value=-1), slice(1, 2, None))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "col_transform = []\n",
    "for i in range(0, 2):\n",
    "    col_transform.append((f'categorical_transformer_{i}',OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1),slice(i,i+1)))\n",
    "\n",
    "print(col_transform)\n",
    "ct = ColumnTransformer(col_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25629336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-30 19:56:24.296774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09f8391071c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcat_data_npa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_data_npa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 delayed(func)(\n\u001b[1;32m    606\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;31m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"use_encoded_value\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mcats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_unique\u001b[0;34m(values, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         return _unique_python(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_unique_python\u001b[0;34m(values, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0muniques_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0muniques_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_extract_missing\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# create set without the missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmissing_values_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_missing_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cat_data_npa = train_df[cat_cols].values\n",
    "print(datetime.datetime.now())\n",
    "ct.fit_transform(cat_data_npa)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92e7e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 16:08:45.726226\n",
      "2022-08-01 16:08:50.066941\n",
      "2022-08-01 16:08:50.067023\n",
      "2022-08-01 16:08:50.128861\n"
     ]
    }
   ],
   "source": [
    "cat_col_transformers = []\n",
    "ctl = []\n",
    "print(datetime.datetime.now())\n",
    "for e in cat_cols:\n",
    "    cat_data_npa = train_df[[e]].values\n",
    "    t = OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1)\n",
    "    cat_col_transformers.append(t)\n",
    "    cat_data_trans_npa = t.fit_transform(cat_data_npa)\n",
    "    ctl.append(cat_data_trans_npa)\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "cat_cols_transformed_npa = np.concatenate(ctl,axis=1)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "176f5a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  1  1  1  0 -1  5  0  6  1  0]\n",
      " [ 2  1  1  1  0 -1  5  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  1  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  2  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]\n",
      " [ 2  2  1  1  0 -1  4  0  6  1  0]]\n"
     ]
    }
   ],
   "source": [
    "print(cat_data_npa_transformed[800:810])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a011b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531451, 11)\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n",
      "auto\n"
     ]
    }
   ],
   "source": [
    "print(cat_data_npa_transformed.shape)\n",
    "for e in col_transformers:\n",
    "    print(e.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_transformers = []\n",
    "ctl = []\n",
    "print(datetime.datetime.now())\n",
    "for e in num_cols:\n",
    "    num_data_npa = train_df[[e]].values\n",
    "    t = OrdinalEncoder(dtype = np.int8, encoded_missing_value = -1)\n",
    "    cat_col_transformers.append(t)\n",
    "    cat_data_trans_npa = t.fit_transform(cat_data_npa)\n",
    "    ctl.append(cat_data_trans_npa)\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "cat_cols_transformed_npa = np.concatenate(ctl,axis=1)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    #train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    train['customer_ID'] = train['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    train.S_2 = pd.to_datetime( train.S_2 )\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(1).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "            \n",
    "    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "    if PAD_CUSTOMER_TO_13_ROWS:\n",
    "        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "        more = np.array([],dtype='int64') \n",
    "        for j in range(1,13):\n",
    "            i = tmp.loc[tmp==j].index.values\n",
    "            more = np.concatenate([more,np.repeat(i,13-j)])\n",
    "        df = train.iloc[:len(more)].copy().fillna(0)\n",
    "        df = df * 0 - 1 #pad numerical columns with -1\n",
    "        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "        df['customer_ID'] = more\n",
    "        train = pd.concat([train,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    train = train[COLS]\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b42d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PROCESSED TRAIN FOLDS AND SAVE TO DISK        \n",
    "print(datetime.datetime.now())\n",
    "for k in range(NUM_FOLDS):\n",
    "\n",
    "    # READ CHUNK OF TRAIN CSV FILE\n",
    "    skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "    train = pd.read_csv(FILENAME_TRAIN_DATA_CSV, nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "    # FEATURE ENGINEER DATAFRAME\n",
    "    train = feature_engineer(train, targets = targets)\n",
    "\n",
    "    # SAVE FILES\n",
    "    print(f'Train_File_{k+1} has {train.customer_ID.nunique()} customers and shape',train.shape)\n",
    "    tar = train[['customer_ID','target']].drop_duplicates().sort_index()\n",
    "    #if not os.path.exists(PATH_TO_PROCESSED_DATA): os.makedirs(PATH_TO_DATA)\n",
    "    tar.to_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k+1}.pqt',index=False)\n",
    "    data = train.iloc[:,1:-1].values.reshape((-1,13,188))\n",
    "    np.save(f'{PATH_TO_PROCESSED_DATA}data_{k+1}',data.astype('float32'))\n",
    "\n",
    "    # CLEAN MEMORY\n",
    "    del train, tar, data\n",
    "    gc.collect()\n",
    "del targets\n",
    "gc.collect()\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b6a51",
   "metadata": {},
   "source": [
    "#### Model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,188))\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10,4)\n",
    "        embeddings.append( emb(inp[:,:,k]) )\n",
    "    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed804f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM LEARNING SCHEUDLE\n",
    "def lrfn(epoch):\n",
    "    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n",
    "    return lr[epoch]\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ad835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPETITION METRIC FROM Konstantin Yakovlev\n",
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56a760",
   "metadata": {},
   "source": [
    "#### Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # SAVE TRUE AND OOF\n",
    "    true = np.array([])\n",
    "    oof = np.array([])\n",
    "    VERBOSE = 2 # use 1 for interactive \n",
    "\n",
    "    for fold in range(5):\n",
    "\n",
    "        # INDICES OF TRAIN AND VALID FOLDS\n",
    "        valid_idx = [2*fold+1, 2*fold+2]\n",
    "        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n",
    "\n",
    "        print('#'*25)\n",
    "        print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "        # READ TRAIN DATA FROM DISK\n",
    "        X_train = []; y_train = []\n",
    "        for k in train_idx:\n",
    "            X_train.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_train.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_train = np.concatenate(X_train,axis=0)\n",
    "        y_train = pd.concat(y_train).target.values\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "\n",
    "        # READ VALID DATA FROM DISK\n",
    "        X_valid = []; y_valid = []\n",
    "        for k in valid_idx:\n",
    "            X_valid.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_valid.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_valid = np.concatenate(X_valid,axis=0)\n",
    "        y_valid = pd.concat(y_valid).target.values\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        h = model.fit(X_train,y_train, \n",
    "                      validation_data = (X_valid,y_valid),\n",
    "                      batch_size=512, epochs=8, verbose=VERBOSE,\n",
    "                      callbacks = [LR])\n",
    "        #if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n",
    "        model.save_weights(f'{PATH_TO_MODEL}gru_fold_{fold+1}.h5')\n",
    "\n",
    "        # INFER VALID DATA\n",
    "        print('Inferring validation data...')\n",
    "        p = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n",
    "\n",
    "        print()\n",
    "        print(f'Fold {fold+1} CV=', amex_metric_mod(y_valid, p) )\n",
    "        print()\n",
    "        true = np.concatenate([true, y_valid])\n",
    "        oof = np.concatenate([oof, p])\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_train, y_train, X_valid, y_valid, p\n",
    "        gc.collect()\n",
    "\n",
    "    # PRINT OVERALL RESULTS\n",
    "    print('#'*25)\n",
    "    print(f'Overall CV =', amex_metric_mod(true, oof) )\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45098d",
   "metadata": {},
   "source": [
    "#### Model evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dd0dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
