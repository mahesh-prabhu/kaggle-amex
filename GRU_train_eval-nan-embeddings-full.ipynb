{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cb4847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 23:12:45.286417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 23:12:45.815944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-01-07 23:12:45.816007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-01-07 23:12:45.816013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "utils.widen_ipython_window()\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bb690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Paths\n",
    "#\n",
    "\n",
    "MAIN_PATH = \"/home/mahesh/Desktop/ML/kaggle/amex/\"\n",
    "\n",
    "# Data\n",
    "PATH_TO_DATA                = MAIN_PATH + \"data/\"\n",
    "PATH_TO_PROCESSED_DATA      = PATH_TO_DATA + \"processed/\"\n",
    "PATH_TO_PROCESSED2_DATA     = PATH_TO_DATA + \"processed2/\"\n",
    "PATH_TO_PROCESSED4_DATA     = PATH_TO_DATA + \"processed4/\"\n",
    "\n",
    "PATH_TO_GRU_NAN_EMBEDDINGS_DATA = PATH_TO_DATA + \"gru_nan_embeddings_full/\"\n",
    "\n",
    "FILENAME_TRAIN_DATA_CSV     = PATH_TO_DATA + \"orig/train_data.csv\"\n",
    "FILENAME_TRAIN_LABELS_CSV   = PATH_TO_DATA + \"orig/train_labels.csv\"\n",
    "FILENAME_TEST_DATA_CSV      = PATH_TO_DATA + \"orig/test_data.csv\"\n",
    "FILENAME_SAMPLE_SUBMISSION_CSV = PATH_TO_DATA + \"orig/sample_submission.csv\"\n",
    "\n",
    "FILENAME_TRAIN_DATA_FEATHER = PATH_TO_PROCESSED_DATA + \"train_data.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_DATA_FEATHER   = PATH_TO_PROCESSED2_DATA + \"train_data.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_LABELS_FEATHER = PATH_TO_PROCESSED2_DATA + \"train_labels.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_DATA_CAT_NOCHANGE_FEATHER   = PATH_TO_PROCESSED2_DATA + \"train_data_cat_nochange.f\"\n",
    "\n",
    "FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_FEATHER = PATH_TO_GRU_NAN_EMBEDDINGS_DATA + \"train_data.f\"\n",
    "FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_NORMALIZED_FEATHER = PATH_TO_GRU_NAN_EMBEDDINGS_DATA + \"train_data_normalized.f\"\n",
    "\n",
    "\n",
    "FILENAME_TEST_CUSTOMER_HASHES  = PATH_TO_PROCESSED2_DATA + \"test_customer_hashes_data.pq\"\n",
    "FILENAME_TEST_HASH_DATA        = PATH_TO_PROCESSED2_DATA + \"test_hashes_data\"\n",
    "FILENAME_GRU_SUBMISSION        = PATH_TO_PROCESSED2_DATA + \"submission_gru.csv\"\n",
    "\n",
    "FILENAME_TRAIN_PROCESSED4_FE_DATA_RNN_FEATHER = PATH_TO_PROCESSED4_DATA + \"train_FE_data_RNN.f\"\n",
    "\n",
    "# Models\n",
    "PATH_TO_MODEL   = PATH_TO_GRU_NAN_EMBEDDINGS_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d996fd",
   "metadata": {},
   "source": [
    "    1. Feature engineer the data\n",
    "    2. Iterate over the folds\n",
    "        a. Load the customer IDs corresponding to the train and val data\n",
    "        b. Extract the data and labels corresponding to the customer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e46223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 20:46:51.915774 : Reading raw data\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531451 entries, 0 to 5531450\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: datetime64[ns](1), float32(176), float64(9), int32(1), int64(1), object(2)\n",
      "memory usage: 4.7 GB\n",
      "2022-09-20 20:46:52.464342 : Starting feature engineering\n",
      "2022-09-20 20:47:22.159561 : NUM_FEATURES : 365, CATS : 11, NUMS_NO_E : 0, NUMS_WITH_E : 354\n",
      "2022-09-20 20:47:22.160803 : Completed feature engineering\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5965869 entries, 0 to 5965868\n",
      "Columns: 367 entries, customer_ID to target\n",
      "dtypes: float32(176), float64(1), int64(1), int8(189)\n",
      "memory usage: 5.1 GB\n",
      "2022-09-20 20:47:22.171760 : Writing feature engineered data to disk\n"
     ]
    }
   ],
   "source": [
    "RUN_FEATURE_ENGINEERING = 1\n",
    "\n",
    "def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None, edit_cid_time = False, num_normalized = False):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    #train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    if edit_cid_time:\n",
    "        train['customer_ID'] = train['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "        train.S_2 = pd.to_datetime( train.S_2 )\n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':1, 'CO':2, 'CR':3, 'XL':4, 'XM':5, 'XZ':6}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(0).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':1,'O':2, 'R':3, 'U':4}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(0).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [1,0,1,1,2,1,2,1,1] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(0).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "\n",
    "    #\n",
    "    # For numerical features that have NaNs add as extra column that indicates if \n",
    "    # the value is not-nan (1) or nan. Then set the nans to zero.\n",
    "    #\n",
    "    NUMS_NO_E = [] # We don't have numerical features without embeddings because we need to pad these features with zeros\n",
    "    NUMS_WITH_E = []\n",
    "    zeros = np.zeros((train.shape[0],), dtype='int8')\n",
    "    ones = np.ones((train.shape[0],), dtype='int8')\n",
    "    for c in train.columns:\n",
    "        if ((c in SKIP) or (c in CATS)): continue\n",
    "        c_exists = c + \"_exists\"\n",
    "        train[c_exists] = np.where(np.invert(np.isnan(train[c].values)), ones, zeros)\n",
    "        #if num_normalized: train[c]=(train[c]-train[c].min())/(train[c].max()-train[c].min())\n",
    "        if num_normalized: train[c]=(train[c]-train[c].mean())/(train[c].std(ddof=0))\n",
    "        train[c] = train[c].fillna(0)\n",
    "        NUMS_WITH_E.append(c)\n",
    "        NUMS_WITH_E.append(c_exists)\n",
    "    #\n",
    "    # Pad data for customers who don't have 13 months of data\n",
    "    #\n",
    "    tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "    more = np.array([],dtype='int64') \n",
    "    for j in range(1,13):\n",
    "        i = tmp.loc[tmp==j].index.values\n",
    "        #print(i)\n",
    "        more = np.concatenate([more,np.repeat(i,13-j)])\n",
    "        #print(more)\n",
    "    df = train.iloc[:len(more)].copy().fillna(0)\n",
    "    df = df * 0\n",
    "    # df = df * 0 - 1 #pad numerical columns with -1\n",
    "    df[CATS] = df[CATS].astype('int8') #pad categorical columns with 0\n",
    "    df['customer_ID'] = more\n",
    "    train = pd.concat([train,df],axis=0,ignore_index=True)\n",
    "\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    #train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    #COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    COLS = ['customer_ID'] + CATS + NUMS_NO_E + NUMS_WITH_E + ['target']\n",
    "    train = train[COLS]\n",
    "    \n",
    "    num_features = len(CATS + NUMS_NO_E + NUMS_WITH_E)\n",
    "    \n",
    "    utils.pt(f'NUM_FEATURES : {num_features}, CATS : {len(CATS)}, NUMS_NO_E : {len(NUMS_NO_E)}, NUMS_WITH_E : {len(NUMS_WITH_E)}' )\n",
    "    \n",
    "    return train\n",
    "\n",
    "def read_train_data():\n",
    "    # Load the data\n",
    "    train_full_data   = pd.read_feather(FILENAME_TRAIN_PROCESSED2_DATA_CAT_NOCHANGE_FEATHER)\n",
    "    train_full_labels = pd.read_feather(FILENAME_TRAIN_PROCESSED2_LABELS_FEATHER)\n",
    "    utils.pt(\"Reading raw data\")\n",
    "    train_full_data.info(memory_usage=\"deep\")\n",
    "    return (train_full_data, train_full_labels)\n",
    "\n",
    "def feature_engineer_full_data_and_save_to_file(train_data_output_filename, num_normalized = False):\n",
    "    (train_full_data, train_full_labels) = read_train_data()\n",
    "    \n",
    "    # Feature engineer\n",
    "    utils.pt(\"Starting feature engineering\")\n",
    "    train_FE_data = feature_engineer(train_full_data, PAD_CUSTOMER_TO_13_ROWS = True, \n",
    "                                     targets = train_full_labels, num_normalized = num_normalized)\n",
    "    utils.pt(\"Completed feature engineering\")\n",
    "    \n",
    "    train_FE_data.info(memory_usage=\"deep\")\n",
    "    \n",
    "    utils.pt(\"Writing feature engineered data to disk\")\n",
    "    train_FE_data.to_feather(train_data_output_filename)\n",
    "    \n",
    "    utils.gc_l([train_full_data, train_full_labels, train_FE_data])\n",
    "    \n",
    "if RUN_FEATURE_ENGINEERING:\n",
    "    #feature_engineer_full_data_and_save_to_file(train_data_output_filename = FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_FEATHER, num_normalized = False)\n",
    "    feature_engineer_full_data_and_save_to_file(train_data_output_filename = FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_NORMALIZED_FEATHER\n",
    "                                                , num_normalized = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b36723",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODELS = 1\n",
    "#\n",
    "# CONFIGS\n",
    "#\n",
    "VERBOSE   = 2\n",
    "SEED      = 42\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "EPOCHS     = 8\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "TARGET_LABEL      = 'target'\n",
    "CUSTOMER_ID_LABEL = \"customer_ID\"\n",
    "\n",
    "CATS = 11\n",
    "NUMS_NO_E = 0\n",
    "NUMS_WITH_E = 354\n",
    "NUM_FEATURES = CATS + NUMS_NO_E + NUMS_WITH_E \n",
    "\n",
    "#NUM_FEATURES = 226\n",
    "\n",
    "\n",
    "\n",
    "def build_gru_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,NUM_FEATURES))\n",
    "    \n",
    "    # input org is {CATS, NUM_NO_E, NUMS_WITH_E}\n",
    "    \n",
    "    # Categorical embeddings\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10,4)\n",
    "        embeddings.append( emb(inp[:,:,k]) )\n",
    "    \n",
    "    # NaN embeddings\n",
    "    ips = []\n",
    "    for k in range (( CATS + NUMS_NO_E), NUM_FEATURES, 2):\n",
    "        l = tf.keras.layers.Dense(1,activation = 'relu')\n",
    "        ips.append(l(inp[:,:,k:k+2]))\n",
    "        \n",
    "    # x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    # x = tf.keras.layers.Concatenate()([inp[:,:,CATS:(CATS+NUMS_NO_E)]] + embeddings + ips)\n",
    "    x = tf.keras.layers.Concatenate()(embeddings + ips)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    x = tf.keras.layers.GRU(units=256, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# CUSTOM LEARNING SCHEUDLE\n",
    "import math\n",
    "TOTAL_EPOCHS = EPOCHS\n",
    "\n",
    "# CUSTOM LEARNING SCHEUDLE\n",
    "def lrfn(epoch):\n",
    "    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n",
    "    i = math.floor(len(lr) * (epoch/TOTAL_EPOCHS))\n",
    "    return lr[i]\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
    "\n",
    "\n",
    "def extract_X_Y(FE_data, cids):\n",
    "    #utils.pt(str(FE_data.shape))\n",
    "    #utils.pt(str(cids.shape))\n",
    "    \n",
    "    data = FE_data.loc[(FE_data.customer_ID.isin(cids.customer_ID.values))]\n",
    "    data = data.reset_index() # this adds \"index\" column to the data-frame\n",
    "    \n",
    "    data.info(memory_usage='deep')\n",
    "    \n",
    "    #utils.pt(str(data.shape))\n",
    "    #data.info()\n",
    "    #utils.pt(str(data.iloc[:,1:-1].values.shape))\n",
    "    #utils.pt(str(data.columns))\n",
    "    \n",
    "    Y = data[['customer_ID','target']].drop_duplicates().sort_index().target.values\n",
    "    utils.pt('Reshaping data')\n",
    "    X = data.iloc[:,2:-1].values.reshape((-1,13, NUM_FEATURES))\n",
    "    \n",
    "    #FE_data.drop((FE_data.customer_ID.isin(cids.customer_ID.values)).index, inplace = True)\n",
    "    \n",
    "    return (X,Y)\n",
    "\n",
    "def extract_val_train_data(fold, train_FE_data):\n",
    "    train_cids = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/train_{CUSTOMER_ID_LABEL}_fold_{fold}.f')\n",
    "    val_cids   = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/val_{CUSTOMER_ID_LABEL}_fold_{fold}.f')\n",
    "    \n",
    "    (X_train, Y_train) = extract_X_Y(train_FE_data, train_cids)\n",
    "    (X_val  , Y_val  ) = extract_X_Y(train_FE_data, val_cids  )\n",
    "    \n",
    "    return (X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "def extract_test_data(train_FE_data):\n",
    "    test_cids = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/test_{CUSTOMER_ID_LABEL}.f')\n",
    "    return (extract_X_Y(train_FE_data, test_cids))\n",
    "    \n",
    "    \n",
    "def train_gru_models():\n",
    "    \n",
    "    utils.pt(f'Reading feature engineered data.')\n",
    "#    train_FE_data = pd.read_feather(FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_FEATHER)\n",
    "    \n",
    "#     utils.pt(f'Extracting test data.')\n",
    "    \n",
    "#     (X_test, Y_test) = extract_test_data(train_FE_data)\n",
    "\n",
    "#     utils.pt(f'### Test data shapes   {X_test.shape} , {Y_test.shape}')\n",
    "    \n",
    "    for fold in range(0,NUM_FOLDS):\n",
    "        \n",
    "        train_FE_data = pd.read_feather(FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_FEATHER)\n",
    "        \n",
    "        utils.pt(f'#### Fold : {fold} ####')\n",
    "        \n",
    "        utils.pt(f'Extracting train and val data.')\n",
    "        (X_train, Y_train, X_val, Y_val) = extract_val_train_data(fold, train_FE_data)\n",
    "        \n",
    "        #utils.gc_l([train_FE_data])\n",
    "        \n",
    "        utils.pt(f'### Training data shapes   {X_train.shape} , {Y_train.shape}')\n",
    "        utils.pt(f'### Validation data shapes {X_val.shape}   , {Y_val.shape}  ')\n",
    "        \n",
    "        \n",
    "        utils.pt(f'Starting model training.')\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_gru_model()\n",
    "        h = model.fit(X_train,Y_train, \n",
    "                      validation_data = (X_val,Y_val),\n",
    "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
    "                      callbacks = [LR])        \n",
    "        utils.pt(f'Completed model training.')\n",
    "        \n",
    "        utils.pt(f'Saving model to file.')\n",
    "        model.save_weights(f'{PATH_TO_MODEL}gru_fold_{fold}.h5')\n",
    "        \n",
    "        # INFER VALID DATA\n",
    "        utils.pt('Inferring validation data...')\n",
    "        preds = model.predict(X_val, batch_size=512, verbose=VERBOSE).flatten()\n",
    "        amm = utils.amex_metric_mod(Y_val, preds)\n",
    "        utils.pt(f'Fold {fold} CV = {amm}')\n",
    "        \n",
    "        \n",
    "        # INFER TEST DATA\n",
    "#         utils.pt('Inferring test data...')\n",
    "#         preds = model.predict(X_test, batch_size=512, verbose=VERBOSE).flatten()\n",
    "#         amm = utils.amex_metric_mod(Y_test, preds)\n",
    "#         utils.pt(f'Fold {fold} CV = {amm}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        utils.gc_l([model, X_train, Y_train, X_val, Y_val, preds, train_FE_data, amm, h])\n",
    "        K.clear_session()\n",
    "        \n",
    "#        train_FE_data = pd.read_feather(FILENAME_TRAIN_PROCESSED_GRU_NAN_EMBEDDINGS_FEATHER)\n",
    "\n",
    "    print()\n",
    "    utils.pt(f'*** Completed model training for all folds ***')\n",
    "\n",
    "if TRAIN_MODELS :\n",
    "    train_gru_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412a52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(customers, train, NUM_FOLDS = 10, verbose = ''):\n",
    "    chunk = len(customers)//NUM_FOLDS\n",
    "    if verbose != '':\n",
    "        utils.pt(f'We will split {verbose} data into {NUM_FOLDS} separate folds.')\n",
    "        utils.pt(f'There will be {chunk} customers in each fold (except the last fold).')\n",
    "        utils.pt('Below are number of rows in each fold:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_FOLDS):\n",
    "        if k==NUM_FOLDS-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': utils.pt( str(rows) )\n",
    "    return rows\n",
    "\n",
    "def get_column_names_cids(read_customer_hashes):\n",
    "    \n",
    "    # GET TEST COLUMN NAMES\n",
    "    test = pd.read_csv(FILENAME_TEST_DATA_CSV, nrows=1)\n",
    "    T_COLS = test.columns\n",
    "    utils.pt(f'There are {len(T_COLS)} test dataframe columns')\n",
    "    \n",
    "    if read_customer_hashes:\n",
    "        test_cids = pd.read_parquet(FILENAME_TEST_CUSTOMER_HASHES)\n",
    "    else:\n",
    "        test_cids = pd.read_csv(FILENAME_TEST_DATA_CSV, usecols=['customer_ID'])\n",
    "        test_cids.to_parquet(FILENAME_TEST_CUSTOMER_HASHES)\n",
    "    \n",
    "    test_cids['customer_ID'] = test_cids['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    cids = test_cids.drop_duplicates().sort_index().values.flatten()\n",
    "    \n",
    "    utils.pt(f'There are {len(cids)} unique customers in test.')\n",
    "    \n",
    "    return (T_COLS, cids, test_cids)\n",
    "\n",
    "def break_up_test_data(T_COLS, NUM_FILES, rows):\n",
    "    \n",
    "    # SAVE TEST CUSTOMERS INDEX\n",
    "    test_customer_hashes = np.array([],dtype='int64')\n",
    "    \n",
    "    # CREATE PROCESSED TEST FILES AND SAVE TO DISK\n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TEST CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        test = pd.read_csv(FILENAME_TEST_DATA_CSV, nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        test = feature_engineer(test, targets = None, edit_cid_time = True)\n",
    "        \n",
    "        # SAVE TEST CUSTOMERS INDEX\n",
    "        cust = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "        test_customer_hashes = np.concatenate([test_customer_hashes,cust])\n",
    "        \n",
    "        # SAVE FILES\n",
    "        utils.pt(f'Test_File_{k+1} has {test.customer_ID.nunique()} customers and shape {test.shape}')\n",
    "        data = test.iloc[:,1:].values.reshape((-1,13,188))\n",
    "        np.save(f'{PATH_TO_PROCESSED2_DATA}test_data_{k+1}',data.astype('float32'))\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        utils.gc_l([test, data])\n",
    "    \n",
    "    # SAVE CUSTOMER INDEX OF ALL TEST FILES\n",
    "    np.save(FILENAME_TEST_HASH_DATA, test_customer_hashes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_TEST_DATA = 0\n",
    "NUM_TEST_FILES = 20\n",
    "READ_CUSTOMER_HASHES = 1\n",
    "\n",
    "if PROCESS_TEST_DATA:\n",
    "    (T_COLS,cids, test_cids) = get_column_names_cids(READ_CUSTOMER_HASHES)\n",
    "    \n",
    "    rows = get_rows(cids, test_cids, NUM_FOLDS = NUM_TEST_FILES, verbose = 'test')\n",
    "    \n",
    "    break_up_test_data(T_COLS, NUM_TEST_FILES, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_TEST_DATA = 1\n",
    "\n",
    "def infer_test_data():\n",
    "    # INFER TEST DATA\n",
    "    start = 0; end = 0\n",
    "    sub = pd.read_csv(FILENAME_SAMPLE_SUBMISSION_CSV)\n",
    "    \n",
    "    # REARANGE SUB ROWS TO MATCH PROCESSED TEST FILES\n",
    "    sub['hash'] = sub['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    test_hash_index = np.load(f'{FILENAME_TEST_HASH_DATA}.npy')\n",
    "    sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n",
    "    \n",
    "    for k in range(NUM_TEST_FILES):\n",
    "        # BUILD MODEL\n",
    "        K.clear_session()\n",
    "        model = build_gru_model()\n",
    "        \n",
    "        # LOAD TEST DATA\n",
    "        utils.pt(f'Inferring Test_File_{k+1}')\n",
    "        X_test = np.load(f'{PATH_TO_PROCESSED2_DATA}test_data_{k+1}.npy')\n",
    "        end = start + X_test.shape[0]\n",
    "\n",
    "        # INFER 5 FOLD MODELS\n",
    "        model.load_weights(f'{PATH_TO_MODEL}gru_fold_0.h5')\n",
    "        p = model.predict(X_test, batch_size=512, verbose=0).flatten() \n",
    "        for j in range(1,5):\n",
    "            model.load_weights(f'{PATH_TO_MODEL}gru_fold_{j}.h5')\n",
    "            p += model.predict(X_test, batch_size=512, verbose=0).flatten()\n",
    "        p /= 5.0\n",
    "\n",
    "        # SAVE TEST PREDICTIONS\n",
    "        sub.loc[start:end-1,'prediction'] = p\n",
    "        start = end\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        utils.gc_l([ model, X_test, p])\n",
    "     \n",
    "    sub.to_csv(FILENAME_GRU_SUBMISSION,index=False)\n",
    "    print('Submission file shape is', sub.shape )\n",
    "    display( sub.head() )\n",
    "\n",
    "    \n",
    "if INFER_TEST_DATA:\n",
    "    infer_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a871bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex-venv",
   "language": "python",
   "name": "amex-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "name": "GRU_train_eval-nan-embeddings-full.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
