{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48d8b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "from keras.regularizers import L2\n",
    "# GPU LIBRARIES, are these useful? Do they result in any meaningful speedup? I have replaced cupy with np and cudf with pd in the below code.\n",
    "#import cupy, cudf \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542619fe",
   "metadata": {},
   "source": [
    "### High level steps\n",
    "\n",
    "**EDA:** See what data we have.  \n",
    "**Data wrangling:** handle N/A, handle categorical data, data normalization for real valued data, not all customers have 13 time data (what to do for these?, is the missing data non contiguous?)  \n",
    "**Feature engineering:** Understand the important features (how to do this? Run a random forest model and see what the key features are?), dimensionality reduction?, drop any features?  \n",
    "**Train, val, test split:** Split data.  \n",
    "**Models:** Start with a simple model, ensembles?, have a mix of simple and advanced algorithms. Use embeddings to express users along latent dimensions with different models and then have an ensemble?, what will be the use of embeddings here?  \n",
    "\n",
    "\n",
    "**Flow:**  \n",
    "feature engineering pipeline -> save/restore data to/from disk (optional depending on how long the feature engineering takes) -> Build models -> evaluate models -> iterate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b732b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Making sure we are using GPU and CUDA for training\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "assert (tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff7c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "MAIN_PATH = \"/home/mahesh/Desktop/ML/kaggle/amex/\"\n",
    "\n",
    "# Data\n",
    "PATH_TO_DATA                = MAIN_PATH + \"data/\"\n",
    "PATH_TO_PROCESSED_DATA      = PATH_TO_DATA + \"processed/\"\n",
    "FILENAME_TRAIN_DATA_CSV     = PATH_TO_DATA + \"orig/train_data.csv\"\n",
    "FILENAME_TRAIN_LABELS_CSV   = PATH_TO_DATA + \"orig/train_labels.csv\"\n",
    "FILENAME_CID_MAP            = PATH_TO_PROCESSED_DATA + \"cid_map.csv\"\n",
    "FILENAME_TRAIN_DATA_FEATHER = PATH_TO_PROCESSED_DATA + \"train_data.f\"\n",
    "\n",
    "# Models\n",
    "PATH_TO_MODEL   = MAIN_PATH + \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a5759",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364052a",
   "metadata": {},
   "source": [
    "#### Model dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e46a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPETITION METRIC FROM Konstantin Yakovlev\n",
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922d7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,188))\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10,4)\n",
    "        embeddings.append( emb(inp[:,:,k]) )\n",
    "    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    #x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n",
    "    \n",
    "    # The returned output should be a batch of sequences.\n",
    "    # x = tf.keras.layers.LSTM(units = 128, return_sequences= True, kernel_regularizer=L2(0.01))(x)\n",
    "    #x = tf.keras.layers.LSTM(units = 256, return_sequences= True)(x)\n",
    "    x = tf.keras.layers.LSTM(units = 512, return_sequences= False)(x)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    # x = tf.keras.layers.Dropout(rate=0.3)(x)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a single hidden state, not a batch of sequences.\n",
    "    #x = tf.keras.layers.LSTM(units = 64, kernel_regularizer=L2(0.01))(x)\n",
    "    # x = tf.keras.layers.LSTM(units = 64)(x)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    # x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    # Propagate X through a Dense layer with 5 units\n",
    "    #x = tf.keras.layers.Dense(units=5)(x)\n",
    "    \n",
    "    #x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41222589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "TOTAL_EPOCHS = 12\n",
    "\n",
    "# CUSTOM LEARNING SCHEUDLE\n",
    "def lrfn(epoch):\n",
    "    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n",
    "    i = math.floor(len(lr) * (epoch/TOTAL_EPOCHS))\n",
    "    return lr[i]\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5352bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-17 14:30:51.452530\n",
      "#########################\n",
      "### Fold 1 with valid files [1, 2]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/12\n",
      "180/180 - 121s - loss: 0.2449 - val_loss: 0.2348 - lr: 0.0010 - 121s/epoch - 672ms/step\n",
      "Epoch 2/12\n",
      "180/180 - 119s - loss: 0.2278 - val_loss: 0.2288 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 3/12\n",
      "180/180 - 119s - loss: 0.2235 - val_loss: 0.2267 - lr: 0.0010 - 119s/epoch - 660ms/step\n",
      "Epoch 4/12\n",
      "180/180 - 119s - loss: 0.2211 - val_loss: 0.2281 - lr: 0.0010 - 119s/epoch - 660ms/step\n",
      "Epoch 5/12\n",
      "180/180 - 119s - loss: 0.2186 - val_loss: 0.2271 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 6/12\n",
      "180/180 - 119s - loss: 0.2164 - val_loss: 0.2264 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 7/12\n",
      "180/180 - 119s - loss: 0.2139 - val_loss: 0.2264 - lr: 0.0010 - 119s/epoch - 660ms/step\n",
      "Epoch 8/12\n",
      "180/180 - 119s - loss: 0.2108 - val_loss: 0.2264 - lr: 0.0010 - 119s/epoch - 659ms/step\n",
      "Epoch 9/12\n",
      "180/180 - 119s - loss: 0.2007 - val_loss: 0.2277 - lr: 1.0000e-04 - 119s/epoch - 660ms/step\n",
      "Epoch 10/12\n",
      "180/180 - 119s - loss: 0.1965 - val_loss: 0.2295 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 11/12\n",
      "180/180 - 119s - loss: 0.1939 - val_loss: 0.2320 - lr: 1.0000e-04 - 119s/epoch - 660ms/step\n",
      "Epoch 12/12\n",
      "180/180 - 119s - loss: 0.1897 - val_loss: 0.2328 - lr: 1.0000e-05 - 119s/epoch - 660ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 10s - 10s/epoch - 58ms/step\n",
      "\n",
      "Fold 1 CV= 0.7773910676928523\n",
      "\n",
      "2022-08-17 14:54:55.434068\n",
      "2022-08-17 14:54:55.434131\n",
      "#########################\n",
      "### Fold 2 with valid files [3, 4]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/12\n",
      "180/180 - 121s - loss: 0.2470 - val_loss: 0.2338 - lr: 0.0010 - 121s/epoch - 674ms/step\n",
      "Epoch 2/12\n",
      "180/180 - 119s - loss: 0.2282 - val_loss: 0.2301 - lr: 0.0010 - 119s/epoch - 663ms/step\n",
      "Epoch 3/12\n",
      "180/180 - 120s - loss: 0.2241 - val_loss: 0.2286 - lr: 0.0010 - 120s/epoch - 665ms/step\n",
      "Epoch 4/12\n",
      "180/180 - 119s - loss: 0.2216 - val_loss: 0.2257 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 5/12\n",
      "180/180 - 119s - loss: 0.2195 - val_loss: 0.2276 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 6/12\n",
      "180/180 - 119s - loss: 0.2171 - val_loss: 0.2268 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 7/12\n",
      "180/180 - 119s - loss: 0.2145 - val_loss: 0.2258 - lr: 0.0010 - 119s/epoch - 663ms/step\n",
      "Epoch 8/12\n",
      "180/180 - 119s - loss: 0.2120 - val_loss: 0.2268 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 9/12\n",
      "180/180 - 119s - loss: 0.2025 - val_loss: 0.2264 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 10/12\n",
      "180/180 - 119s - loss: 0.1988 - val_loss: 0.2278 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 11/12\n",
      "180/180 - 119s - loss: 0.1962 - val_loss: 0.2295 - lr: 1.0000e-04 - 119s/epoch - 660ms/step\n",
      "Epoch 12/12\n",
      "180/180 - 119s - loss: 0.1926 - val_loss: 0.2302 - lr: 1.0000e-05 - 119s/epoch - 661ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 10s - 10s/epoch - 57ms/step\n",
      "\n",
      "Fold 2 CV= 0.7766655665762552\n",
      "\n",
      "2022-08-17 15:19:02.640218\n",
      "2022-08-17 15:19:02.640290\n",
      "#########################\n",
      "### Fold 3 with valid files [5, 6]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/12\n",
      "180/180 - 121s - loss: 0.2461 - val_loss: 0.2318 - lr: 0.0010 - 121s/epoch - 673ms/step\n",
      "Epoch 2/12\n",
      "180/180 - 119s - loss: 0.2286 - val_loss: 0.2265 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 3/12\n",
      "180/180 - 119s - loss: 0.2249 - val_loss: 0.2269 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 4/12\n",
      "180/180 - 119s - loss: 0.2221 - val_loss: 0.2241 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 5/12\n",
      "180/180 - 119s - loss: 0.2193 - val_loss: 0.2269 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 6/12\n",
      "180/180 - 119s - loss: 0.2174 - val_loss: 0.2234 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 7/12\n",
      "180/180 - 119s - loss: 0.2144 - val_loss: 0.2240 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 8/12\n",
      "180/180 - 119s - loss: 0.2118 - val_loss: 0.2255 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 9/12\n",
      "180/180 - 119s - loss: 0.2021 - val_loss: 0.2254 - lr: 1.0000e-04 - 119s/epoch - 662ms/step\n",
      "Epoch 10/12\n",
      "180/180 - 119s - loss: 0.1981 - val_loss: 0.2275 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 11/12\n",
      "180/180 - 119s - loss: 0.1954 - val_loss: 0.2290 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 12/12\n",
      "180/180 - 119s - loss: 0.1916 - val_loss: 0.2297 - lr: 1.0000e-05 - 119s/epoch - 662ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 10s - 10s/epoch - 57ms/step\n",
      "\n",
      "Fold 3 CV= 0.7766670997051589\n",
      "\n",
      "2022-08-17 15:43:09.146147\n",
      "2022-08-17 15:43:09.146213\n",
      "#########################\n",
      "### Fold 4 with valid files [7, 8]\n",
      "### Training data shapes (367131, 13, 188) (367131,)\n",
      "### Validation data shapes (91782, 13, 188) (91782,)\n",
      "#########################\n",
      "Epoch 1/12\n",
      "180/180 - 121s - loss: 0.2481 - val_loss: 0.2296 - lr: 0.0010 - 121s/epoch - 673ms/step\n",
      "Epoch 2/12\n",
      "180/180 - 119s - loss: 0.2286 - val_loss: 0.2264 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 3/12\n",
      "180/180 - 119s - loss: 0.2248 - val_loss: 0.2266 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 4/12\n",
      "180/180 - 119s - loss: 0.2222 - val_loss: 0.2234 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 5/12\n",
      "180/180 - 119s - loss: 0.2197 - val_loss: 0.2231 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 6/12\n",
      "180/180 - 119s - loss: 0.2177 - val_loss: 0.2223 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 7/12\n",
      "180/180 - 119s - loss: 0.2150 - val_loss: 0.2233 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 8/12\n",
      "180/180 - 119s - loss: 0.2124 - val_loss: 0.2272 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 9/12\n",
      "180/180 - 119s - loss: 0.2024 - val_loss: 0.2236 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 10/12\n",
      "180/180 - 119s - loss: 0.1984 - val_loss: 0.2249 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 11/12\n",
      "180/180 - 119s - loss: 0.1956 - val_loss: 0.2263 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 12/12\n",
      "180/180 - 119s - loss: 0.1918 - val_loss: 0.2273 - lr: 1.0000e-05 - 119s/epoch - 662ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 10s - 10s/epoch - 57ms/step\n",
      "\n",
      "Fold 4 CV= 0.7794790523985627\n",
      "\n",
      "2022-08-17 16:07:15.323167\n",
      "2022-08-17 16:07:15.323232\n",
      "#########################\n",
      "### Fold 5 with valid files [9, 10]\n",
      "### Training data shapes (367128, 13, 188) (367128,)\n",
      "### Validation data shapes (91785, 13, 188) (91785,)\n",
      "#########################\n",
      "Epoch 1/12\n",
      "180/180 - 121s - loss: 0.2483 - val_loss: 0.2284 - lr: 0.0010 - 121s/epoch - 674ms/step\n",
      "Epoch 2/12\n",
      "180/180 - 119s - loss: 0.2288 - val_loss: 0.2297 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 3/12\n",
      "180/180 - 119s - loss: 0.2250 - val_loss: 0.2221 - lr: 0.0010 - 119s/epoch - 663ms/step\n",
      "Epoch 4/12\n",
      "180/180 - 119s - loss: 0.2221 - val_loss: 0.2228 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 5/12\n",
      "180/180 - 119s - loss: 0.2206 - val_loss: 0.2270 - lr: 0.0010 - 119s/epoch - 661ms/step\n",
      "Epoch 6/12\n",
      "180/180 - 119s - loss: 0.2182 - val_loss: 0.2220 - lr: 0.0010 - 119s/epoch - 663ms/step\n",
      "Epoch 7/12\n",
      "180/180 - 119s - loss: 0.2152 - val_loss: 0.2218 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 8/12\n",
      "180/180 - 119s - loss: 0.2125 - val_loss: 0.2224 - lr: 0.0010 - 119s/epoch - 662ms/step\n",
      "Epoch 9/12\n",
      "180/180 - 119s - loss: 0.2030 - val_loss: 0.2229 - lr: 1.0000e-04 - 119s/epoch - 661ms/step\n",
      "Epoch 10/12\n",
      "180/180 - 119s - loss: 0.1992 - val_loss: 0.2249 - lr: 1.0000e-04 - 119s/epoch - 662ms/step\n",
      "Epoch 11/12\n",
      "180/180 - 119s - loss: 0.1965 - val_loss: 0.2270 - lr: 1.0000e-04 - 119s/epoch - 662ms/step\n",
      "Epoch 12/12\n",
      "180/180 - 119s - loss: 0.1928 - val_loss: 0.2271 - lr: 1.0000e-05 - 119s/epoch - 662ms/step\n",
      "Inferring validation data...\n",
      "180/180 - 10s - 10s/epoch - 57ms/step\n",
      "\n",
      "Fold 5 CV= 0.78075434412966\n",
      "\n",
      "2022-08-17 16:31:22.896566\n",
      "#########################\n",
      "Overall CV = 0.778177391743927\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # SAVE TRUE AND OOF\n",
    "    true = np.array([])\n",
    "    oof = np.array([])\n",
    "    VERBOSE = 2 # use 1 for interactive \n",
    "\n",
    "    for fold in range(5):\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # INDICES OF TRAIN AND VALID FOLDS\n",
    "        valid_idx = [2*fold+1, 2*fold+2]\n",
    "        train_idx = [x for x in [1,2,3,4,5,6,7,8,9,10] if x not in valid_idx]\n",
    "\n",
    "        print('#'*25)\n",
    "        print(f'### Fold {fold+1} with valid files', valid_idx)\n",
    "\n",
    "        # READ TRAIN DATA FROM DISK\n",
    "        X_train = []; y_train = []\n",
    "        for k in train_idx:\n",
    "            X_train.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_train.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_train = np.concatenate(X_train,axis=0)\n",
    "        y_train = pd.concat(y_train).target.values\n",
    "        print('### Training data shapes', X_train.shape, y_train.shape)\n",
    "\n",
    "        # READ VALID DATA FROM DISK\n",
    "        X_valid = []; y_valid = []\n",
    "        for k in valid_idx:\n",
    "            X_valid.append( np.load(f'{PATH_TO_PROCESSED_DATA}data_{k}.npy'))\n",
    "            y_valid.append( pd.read_parquet(f'{PATH_TO_PROCESSED_DATA}targets_{k}.pqt') )\n",
    "        X_valid = np.concatenate(X_valid,axis=0)\n",
    "        y_valid = pd.concat(y_valid).target.values\n",
    "        print('### Validation data shapes', X_valid.shape, y_valid.shape)\n",
    "        print('#'*25)\n",
    "\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        BATCH_SIZE = 2048\n",
    "        h = model.fit(X_train,y_train, \n",
    "                      validation_data = (X_valid,y_valid),\n",
    "                      batch_size=BATCH_SIZE, epochs=TOTAL_EPOCHS, verbose=VERBOSE,\n",
    "                      callbacks = [LR])\n",
    "        #if not os.path.exists(PATH_TO_MODEL): os.makedirs(PATH_TO_MODEL)\n",
    "        model.save_weights(f'{PATH_TO_MODEL}LSTM_fold_{fold+1}.h5')\n",
    "\n",
    "        # INFER VALID DATA\n",
    "        print('Inferring validation data...')\n",
    "        p = model.predict(X_valid, batch_size=512, verbose=VERBOSE).flatten()\n",
    "\n",
    "        print()\n",
    "        print(f'Fold {fold+1} CV=', amex_metric_mod(y_valid, p) )\n",
    "        print()\n",
    "        true = np.concatenate([true, y_valid])\n",
    "        oof = np.concatenate([oof, p])\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        del model, X_train, y_train, X_valid, y_valid, p\n",
    "        gc.collect()\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "    # PRINT OVERALL RESULTS\n",
    "    print('#'*25)\n",
    "    print(f'Overall CV =', amex_metric_mod(true, oof) )\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144cc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
