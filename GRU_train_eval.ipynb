{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cb4847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "utils.widen_ipython_window()\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bb690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Paths\n",
    "#\n",
    "\n",
    "MAIN_PATH = \"/home/mahesh/Desktop/ML/kaggle/amex/\"\n",
    "\n",
    "# Data\n",
    "PATH_TO_DATA                = MAIN_PATH + \"data/\"\n",
    "PATH_TO_PROCESSED_DATA      = PATH_TO_DATA + \"processed/\"\n",
    "PATH_TO_PROCESSED2_DATA     = PATH_TO_DATA + \"processed2/\"\n",
    "PATH_TO_PROCESSED4_DATA     = PATH_TO_DATA + \"processed4/\"\n",
    "\n",
    "FILENAME_TRAIN_DATA_CSV     = PATH_TO_DATA + \"orig/train_data.csv\"\n",
    "FILENAME_TRAIN_LABELS_CSV   = PATH_TO_DATA + \"orig/train_labels.csv\"\n",
    "FILENAME_TEST_DATA_CSV      = PATH_TO_DATA + \"orig/test_data.csv\"\n",
    "FILENAME_SAMPLE_SUBMISSION_CSV = PATH_TO_DATA + \"orig/sample_submission.csv\"\n",
    "\n",
    "FILENAME_TRAIN_DATA_FEATHER = PATH_TO_PROCESSED_DATA + \"train_data.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_DATA_FEATHER   = PATH_TO_PROCESSED2_DATA + \"train_data.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_LABELS_FEATHER = PATH_TO_PROCESSED2_DATA + \"train_labels.f\"\n",
    "FILENAME_TRAIN_PROCESSED2_DATA_CAT_NOCHANGE_FEATHER   = PATH_TO_PROCESSED2_DATA + \"train_data_cat_nochange.f\"\n",
    "\n",
    "FILENAME_TEST_CUSTOMER_HASHES  = PATH_TO_PROCESSED2_DATA + \"test_customer_hashes_data.pq\"\n",
    "FILENAME_TEST_HASH_DATA        = PATH_TO_PROCESSED2_DATA + \"test_hashes_data\"\n",
    "FILENAME_GRU_SUBMISSION        = PATH_TO_PROCESSED2_DATA + \"submission_gru.csv\"\n",
    "\n",
    "FILENAME_TRAIN_PROCESSED4_FE_DATA_RNN_FEATHER = PATH_TO_PROCESSED4_DATA + \"train_FE_data_RNN.f\"\n",
    "\n",
    "# Models\n",
    "PATH_TO_MODEL   = MAIN_PATH + \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d996fd",
   "metadata": {},
   "source": [
    "    1. Feature engineer the data\n",
    "    2. Iterate over the folds\n",
    "        a. Load the customer IDs corresponding to the train and val data\n",
    "        b. Extract the data and labels corresponding to the customer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e46223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FEATURE_ENGINEERING = 0\n",
    "\n",
    "def feature_engineer(train, PAD_CUSTOMER_TO_13_ROWS = True, targets = None, edit_cid_time = False):\n",
    "        \n",
    "    # REDUCE STRING COLUMNS \n",
    "    # from 64 bytes to 8 bytes, and 10 bytes to 3 bytes respectively\n",
    "    #train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    if edit_cid_time:\n",
    "        train['customer_ID'] = train['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "        train.S_2 = pd.to_datetime( train.S_2 )\n",
    "    train['year'] = (train.S_2.dt.year-2000).astype('int8')\n",
    "    train['month'] = (train.S_2.dt.month).astype('int8')\n",
    "    train['day'] = (train.S_2.dt.day).astype('int8')\n",
    "    del train['S_2']\n",
    "        \n",
    "    # LABEL ENCODE CAT COLUMNS (and reduce to 1 byte)\n",
    "    # with 0: padding, 1: nan, 2,3,4,etc: values\n",
    "    d_63_map = {'CL':2, 'CO':3, 'CR':4, 'XL':5, 'XM':6, 'XZ':7}\n",
    "    train['D_63'] = train.D_63.map(d_63_map).fillna(1).astype('int8')\n",
    "\n",
    "    d_64_map = {'-1':2,'O':3, 'R':4, 'U':5}\n",
    "    train['D_64'] = train.D_64.map(d_64_map).fillna(1).astype('int8')\n",
    "    \n",
    "    CATS = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68']\n",
    "    OFFSETS = [2,1,2,2,3,2,3,2,2] #2 minus minimal value in full train csv\n",
    "    # then 0 will be padding, 1 will be NAN, 2,3,4,etc will be values\n",
    "    for c,s in zip(CATS,OFFSETS):\n",
    "        train[c] = train[c] + s\n",
    "        train[c] = train[c].fillna(1).astype('int8')\n",
    "    CATS += ['D_63','D_64']\n",
    "    \n",
    "    # ADD NEW FEATURES HERE\n",
    "    # EXAMPLE: train['feature_189'] = etc etc etc\n",
    "    # EXAMPLE: train['feature_190'] = etc etc etc\n",
    "    # IF CATEGORICAL, THEN ADD TO CATS WITH: CATS += ['feaure_190'] etc etc etc\n",
    "    \n",
    "    # REDUCE MEMORY DTYPE\n",
    "    SKIP = ['customer_ID','year','month','day']\n",
    "    for c in train.columns:\n",
    "        if c in SKIP: continue\n",
    "        if str( train[c].dtype )=='int64':\n",
    "            train[c] = train[c].astype('int32')\n",
    "        if str( train[c].dtype )=='float64':\n",
    "            train[c] = train[c].astype('float32')\n",
    "            \n",
    "    # PAD ROWS SO EACH CUSTOMER HAS 13 ROWS\n",
    "    if PAD_CUSTOMER_TO_13_ROWS:\n",
    "        tmp = train[['customer_ID']].groupby('customer_ID').customer_ID.agg('count')\n",
    "        more = np.array([],dtype='int64') \n",
    "        for j in range(1,13):\n",
    "            i = tmp.loc[tmp==j].index.values\n",
    "            more = np.concatenate([more,np.repeat(i,13-j)])\n",
    "        df = train.iloc[:len(more)].copy().fillna(0)\n",
    "        df = df * 0 - 1 #pad numerical columns with -1\n",
    "        df[CATS] = (df[CATS] * 0).astype('int8') #pad categorical columns with 0\n",
    "        df['customer_ID'] = more\n",
    "        train = pd.concat([train,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    # ADD TARGETS (and reduce to 1 byte)\n",
    "    if targets is not None:\n",
    "        train = train.merge(targets,on='customer_ID',how='left')\n",
    "        train.target = train.target.astype('int8')\n",
    "        \n",
    "    # FILL NAN\n",
    "    train = train.fillna(-0.5) #this applies to numerical columns\n",
    "    \n",
    "    # SORT BY CUSTOMER THEN DATE\n",
    "    train = train.sort_values(['customer_ID','year','month','day']).reset_index(drop=True)\n",
    "    train = train.drop(['year','month','day'],axis=1)\n",
    "    \n",
    "    # REARRANGE COLUMNS WITH 11 CATS FIRST\n",
    "    COLS = list(train.columns[1:])\n",
    "    COLS = ['customer_ID'] + CATS + [c for c in COLS if c not in CATS]\n",
    "    train = train[COLS]\n",
    "    \n",
    "    return train\n",
    "\n",
    "def read_train_data():\n",
    "    # Load the data\n",
    "    train_full_data   = pd.read_feather(FILENAME_TRAIN_PROCESSED2_DATA_CAT_NOCHANGE_FEATHER)\n",
    "    train_full_labels = pd.read_feather(FILENAME_TRAIN_PROCESSED2_LABELS_FEATHER)\n",
    "    utils.pt(\"Reading raw data\")\n",
    "    train_full_data.info(memory_usage=\"deep\")\n",
    "    return (train_full_data, train_full_labels)\n",
    "\n",
    "def feature_engineer_full_data_and_save_to_file():\n",
    "    (train_full_data, train_full_labels) = read_train_data()\n",
    "    \n",
    "    # Feature engineer\n",
    "    utils.pt(\"Starting feature engineering\")\n",
    "    train_FE_data = feature_engineer(train_full_data, PAD_CUSTOMER_TO_13_ROWS = True, targets = train_full_labels)\n",
    "    utils.pt(\"Completed feature engineering\")\n",
    "    \n",
    "    train_FE_data.info(memory_usage=\"deep\")\n",
    "    \n",
    "    utils.pt(\"Writing feature engineered data to disk\")\n",
    "    train_FE_data.to_feather(FILENAME_TRAIN_PROCESSED4_FE_DATA_RNN_FEATHER)\n",
    "    \n",
    "    utils.gc_l([train_full_data, train_full_labels, train_FE_data])\n",
    "    \n",
    "if RUN_FEATURE_ENGINEERING:\n",
    "    feature_engineer_full_data_and_save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b36723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 17:34:33.773624 : Reading feature engineered data.\n",
      "2022-08-29 17:34:35.990073 : Extracting test data.\n",
      "2022-08-29 17:34:36.330665 : ### Test data shapes   (22946, 13, 188) , (22946,)\n",
      "2022-08-29 17:34:36.330736 : #### Fold -0 ####\n",
      "2022-08-29 17:34:36.330750 : Extracting train and val data.\n",
      "2022-08-29 17:34:41.595207 : ### Training data shapes   (348773, 13, 188) , (348773,)\n",
      "2022-08-29 17:34:41.595355 : ### Validation data shapes (87194, 13, 188)   , (87194,)  \n",
      "2022-08-29 17:34:41.595368 : Starting model training.\n",
      "Epoch 1/8\n",
      "682/682 - 10s - loss: 0.2443 - val_loss: 0.2274 - lr: 0.0010 - 10s/epoch - 14ms/step\n",
      "Epoch 2/8\n",
      "682/682 - 6s - loss: 0.2291 - val_loss: 0.2234 - lr: 0.0010 - 6s/epoch - 8ms/step\n",
      "Epoch 3/8\n",
      "682/682 - 5s - loss: 0.2263 - val_loss: 0.2209 - lr: 0.0010 - 5s/epoch - 8ms/step\n",
      "Epoch 4/8\n",
      "682/682 - 6s - loss: 0.2235 - val_loss: 0.2219 - lr: 0.0010 - 6s/epoch - 8ms/step\n",
      "Epoch 5/8\n",
      "682/682 - 6s - loss: 0.2210 - val_loss: 0.2200 - lr: 0.0010 - 6s/epoch - 8ms/step\n",
      "Epoch 6/8\n",
      "682/682 - 6s - loss: 0.2151 - val_loss: 0.2179 - lr: 1.0000e-04 - 6s/epoch - 8ms/step\n",
      "Epoch 7/8\n",
      "682/682 - 6s - loss: 0.2139 - val_loss: 0.2179 - lr: 1.0000e-04 - 6s/epoch - 8ms/step\n",
      "Epoch 8/8\n",
      "682/682 - 6s - loss: 0.2127 - val_loss: 0.2177 - lr: 1.0000e-05 - 6s/epoch - 8ms/step\n",
      "2022-08-29 17:35:37.051639 : Completed model training.\n",
      "2022-08-29 17:35:37.051714 : Saving model to file.\n",
      "2022-08-29 17:35:37.081347 : Inferring validation data...\n",
      "171/171 - 1s - 971ms/epoch - 6ms/step\n",
      "2022-08-29 17:35:39.310003 : Fold 0 CV = 0.7906734090372414\n",
      "2022-08-29 17:35:39.310060 : Inferring test data...\n",
      "45/45 - 0s - 132ms/epoch - 3ms/step\n",
      "2022-08-29 17:35:39.793621 : Fold 0 CV = 0.7908513108719281\n",
      "\n",
      "2022-08-29 17:35:39.899478 : #### Fold -1 ####\n",
      "2022-08-29 17:35:39.899541 : Extracting train and val data.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODELS = 1\n",
    "#\n",
    "# CONFIGS\n",
    "#\n",
    "VERBOSE   = 2\n",
    "SEED      = 42\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "EPOCHS     = 8\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "TARGET_LABEL      = 'target'\n",
    "CUSTOMER_ID_LABEL = \"customer_ID\"\n",
    "\n",
    "\n",
    "\n",
    "def build_gru_model():\n",
    "    \n",
    "    # INPUT - FIRST 11 COLUMNS ARE CAT, NEXT 177 ARE NUMERIC\n",
    "    inp = tf.keras.Input(shape=(13,188))\n",
    "    embeddings = []\n",
    "    for k in range(11):\n",
    "        emb = tf.keras.layers.Embedding(10,4)\n",
    "        embeddings.append( emb(inp[:,:,k]) )\n",
    "    x = tf.keras.layers.Concatenate()([inp[:,:,11:]]+embeddings)\n",
    "    \n",
    "    # SIMPLE RNN BACKBONE\n",
    "    x = tf.keras.layers.GRU(units=128, return_sequences=False)(x)\n",
    "    x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "    \n",
    "    # OUTPUT\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    # COMPILE MODEL\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer = opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# CUSTOM LEARNING SCHEUDLE\n",
    "def lrfn(epoch):\n",
    "    lr = [1e-3]*5 + [1e-4]*2 + [1e-5]*1\n",
    "    return lr[epoch]\n",
    "LR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
    "\n",
    "\n",
    "def extract_X_Y(FE_data, cids):\n",
    "    #utils.pt(str(FE_data.shape))\n",
    "    #utils.pt(str(cids.shape))\n",
    "    \n",
    "    data = FE_data.loc[(FE_data.customer_ID.isin(cids.customer_ID.values))]\n",
    "    data = data.reset_index() # this adds \"index\" column to the data-frame\n",
    "    \n",
    "    #utils.pt(str(data.shape))\n",
    "    #data.info()\n",
    "    #utils.pt(str(data.iloc[:,1:-1].values.shape))\n",
    "    #utils.pt(str(data.columns))\n",
    "    \n",
    "    Y = data[['customer_ID','target']].drop_duplicates().sort_index().target.values\n",
    "    X = data.iloc[:,2:-1].values.reshape((-1,13,188))\n",
    "    \n",
    "    return (X,Y)\n",
    "\n",
    "def extract_val_train_data(fold, train_FE_data):\n",
    "    train_cids = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/train_{CUSTOMER_ID_LABEL}_fold_{fold}.f')\n",
    "    val_cids   = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/val_{CUSTOMER_ID_LABEL}_fold_{fold}.f')\n",
    "    \n",
    "    (X_train, Y_train) = extract_X_Y(train_FE_data, train_cids) \n",
    "    (X_val  , Y_val  ) = extract_X_Y(train_FE_data, val_cids  )\n",
    "    \n",
    "    return (X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "def extract_test_data(train_FE_data):\n",
    "    test_cids = pd.read_feather(f'{PATH_TO_PROCESSED4_DATA}/test_{CUSTOMER_ID_LABEL}.f')\n",
    "    return (extract_X_Y(train_FE_data, test_cids))\n",
    "    \n",
    "    \n",
    "def train_gru_models():\n",
    "    \n",
    "    utils.pt(f'Reading feature engineered data.')\n",
    "    train_FE_data = pd.read_feather(FILENAME_TRAIN_PROCESSED4_FE_DATA_RNN_FEATHER)\n",
    "    \n",
    "    utils.pt(f'Extracting test data.')\n",
    "    \n",
    "    (X_test, Y_test) = extract_test_data(train_FE_data)\n",
    "\n",
    "    utils.pt(f'### Test data shapes   {X_test.shape} , {Y_test.shape}')\n",
    "    \n",
    "    for fold in range(0,NUM_FOLDS):\n",
    "        \n",
    "        utils.pt(f'#### Fold -{fold} ####')\n",
    "        \n",
    "        utils.pt(f'Extracting train and val data.')\n",
    "        (X_train, Y_train, X_val, Y_val) = extract_val_train_data(fold, train_FE_data)\n",
    "        \n",
    "        utils.pt(f'### Training data shapes   {X_train.shape} , {Y_train.shape}')\n",
    "        utils.pt(f'### Validation data shapes {X_val.shape}   , {Y_val.shape}  ')\n",
    "        \n",
    "        utils.pt(f'Starting model training.')\n",
    "        # BUILD AND TRAIN MODEL\n",
    "        K.clear_session()\n",
    "        model = build_gru_model()\n",
    "        h = model.fit(X_train,Y_train, \n",
    "                      validation_data = (X_val,Y_val),\n",
    "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
    "                      callbacks = [LR])        \n",
    "        utils.pt(f'Completed model training.')\n",
    "        \n",
    "        utils.pt(f'Saving model to file.')\n",
    "        model.save_weights(f'{PATH_TO_MODEL}gru_fold_{fold}.h5')\n",
    "        \n",
    "        # INFER VALID DATA\n",
    "        utils.pt('Inferring validation data...')\n",
    "        preds = model.predict(X_val, batch_size=512, verbose=VERBOSE).flatten()\n",
    "        amm = utils.amex_metric_mod(Y_val, preds)\n",
    "        utils.pt(f'Fold {fold} CV = {amm}')\n",
    "        \n",
    "        \n",
    "        # INFER TEST DATA\n",
    "        utils.pt('Inferring test data...')\n",
    "        preds = model.predict(X_test, batch_size=512, verbose=VERBOSE).flatten()\n",
    "        amm = utils.amex_metric_mod(Y_test, preds)\n",
    "        utils.pt(f'Fold {fold} CV = {amm}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        utils.gc_l([model, X_train, Y_train, X_val, Y_val, preds])\n",
    "\n",
    "    print()\n",
    "    utils.pt(f'*** Completed model training for all folds ***')\n",
    "\n",
    "if TRAIN_MODELS :\n",
    "    train_gru_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d95bf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(customers, train, NUM_FOLDS = 10, verbose = ''):\n",
    "    chunk = len(customers)//NUM_FOLDS\n",
    "    if verbose != '':\n",
    "        utils.pt(f'We will split {verbose} data into {NUM_FOLDS} separate folds.')\n",
    "        utils.pt(f'There will be {chunk} customers in each fold (except the last fold).')\n",
    "        utils.pt('Below are number of rows in each fold:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_FOLDS):\n",
    "        if k==NUM_FOLDS-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = train.loc[train.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': utils.pt( str(rows) )\n",
    "    return rows\n",
    "\n",
    "def get_column_names_cids(read_customer_hashes):\n",
    "    \n",
    "    # GET TEST COLUMN NAMES\n",
    "    test = pd.read_csv(FILENAME_TEST_DATA_CSV, nrows=1)\n",
    "    T_COLS = test.columns\n",
    "    utils.pt(f'There are {len(T_COLS)} test dataframe columns')\n",
    "    \n",
    "    if read_customer_hashes:\n",
    "        test_cids = pd.read_parquet(FILENAME_TEST_CUSTOMER_HASHES)\n",
    "    else:\n",
    "        test_cids = pd.read_csv(FILENAME_TEST_DATA_CSV, usecols=['customer_ID'])\n",
    "        test_cids.to_parquet(FILENAME_TEST_CUSTOMER_HASHES)\n",
    "    \n",
    "    test_cids['customer_ID'] = test_cids['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    cids = test_cids.drop_duplicates().sort_index().values.flatten()\n",
    "    \n",
    "    utils.pt(f'There are {len(cids)} unique customers in test.')\n",
    "    \n",
    "    return (T_COLS, cids, test_cids)\n",
    "\n",
    "def break_up_test_data(T_COLS, NUM_FILES, rows):\n",
    "    \n",
    "    # SAVE TEST CUSTOMERS INDEX\n",
    "    test_customer_hashes = np.array([],dtype='int64')\n",
    "    \n",
    "    # CREATE PROCESSED TEST FILES AND SAVE TO DISK\n",
    "    for k in range(NUM_FILES):\n",
    "\n",
    "        # READ CHUNK OF TEST CSV FILE\n",
    "        skip = int(np.sum( rows[:k] ) + 1) #the plus one is for skipping header\n",
    "        test = pd.read_csv(FILENAME_TEST_DATA_CSV, nrows=rows[k], \n",
    "                              skiprows=skip, header=None, names=T_COLS)\n",
    "\n",
    "        # FEATURE ENGINEER DATAFRAME\n",
    "        test = feature_engineer(test, targets = None, edit_cid_time = True)\n",
    "        \n",
    "        # SAVE TEST CUSTOMERS INDEX\n",
    "        cust = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "        test_customer_hashes = np.concatenate([test_customer_hashes,cust])\n",
    "        \n",
    "        # SAVE FILES\n",
    "        utils.pt(f'Test_File_{k+1} has {test.customer_ID.nunique()} customers and shape {test.shape}')\n",
    "        data = test.iloc[:,1:].values.reshape((-1,13,188))\n",
    "        np.save(f'{PATH_TO_PROCESSED2_DATA}test_data_{k+1}',data.astype('float32'))\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        utils.gc_l([test, data])\n",
    "    \n",
    "    # SAVE CUSTOMER INDEX OF ALL TEST FILES\n",
    "    np.save(FILENAME_TEST_HASH_DATA, test_customer_hashes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14f4596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-24 13:53:07.632611 : There are 190 test dataframe columns\n",
      "2022-08-24 13:53:15.475086 : There are 924621 unique customers in test.\n",
      "2022-08-24 13:53:15.475487 : We will split test data into 20 separate folds.\n",
      "2022-08-24 13:53:15.475511 : There will be 46231 customers in each fold (except the last fold).\n",
      "2022-08-24 13:53:15.475523 : Below are number of rows in each fold:\n",
      "2022-08-24 13:53:16.997456 : [567933, 568482, 569369, 567886, 567539, 568041, 568138, 567596, 568543, 567539, 568421, 568745, 568279, 568333, 568327, 568901, 568300, 568001, 567372, 568017]\n",
      "2022-08-24 13:53:40.155512 : Test_File_1 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:54:11.279151 : Test_File_2 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:54:47.048248 : Test_File_3 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:55:24.140497 : Test_File_4 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:56:05.133124 : Test_File_5 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:56:51.514159 : Test_File_6 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:57:40.347294 : Test_File_7 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:58:35.922187 : Test_File_8 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 13:59:34.877919 : Test_File_9 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:00:40.842004 : Test_File_10 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:01:49.826799 : Test_File_11 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:03:05.998673 : Test_File_12 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:04:20.035354 : Test_File_13 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:05:38.308955 : Test_File_14 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:06:58.815544 : Test_File_15 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:08:23.501642 : Test_File_16 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:09:54.645364 : Test_File_17 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:11:34.423441 : Test_File_18 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:13:30.668029 : Test_File_19 has 46231 customers and shape (601003, 189)\n",
      "2022-08-24 14:15:36.758867 : Test_File_20 has 46232 customers and shape (601016, 189)\n"
     ]
    }
   ],
   "source": [
    "PROCESS_TEST_DATA = 0\n",
    "NUM_TEST_FILES = 20\n",
    "READ_CUSTOMER_HASHES = 1\n",
    "\n",
    "if PROCESS_TEST_DATA:\n",
    "    (T_COLS,cids, test_cids) = get_column_names_cids(READ_CUSTOMER_HASHES)\n",
    "    \n",
    "    rows = get_rows(cids, test_cids, NUM_FOLDS = NUM_TEST_FILES, verbose = 'test')\n",
    "    \n",
    "    break_up_test_data(T_COLS, NUM_TEST_FILES, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a052a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-24 14:18:31.977190 : Inferring Test_File_1\n",
      "2022-08-24 14:18:38.203911 : Inferring Test_File_2\n",
      "2022-08-24 14:18:42.979605 : Inferring Test_File_3\n",
      "2022-08-24 14:18:47.433188 : Inferring Test_File_4\n",
      "2022-08-24 14:18:52.029295 : Inferring Test_File_5\n",
      "2022-08-24 14:18:56.513967 : Inferring Test_File_6\n",
      "2022-08-24 14:19:01.364393 : Inferring Test_File_7\n",
      "2022-08-24 14:19:05.891329 : Inferring Test_File_8\n",
      "2022-08-24 14:19:10.600180 : Inferring Test_File_9\n",
      "2022-08-24 14:19:15.565886 : Inferring Test_File_10\n",
      "2022-08-24 14:19:20.203828 : Inferring Test_File_11\n",
      "2022-08-24 14:19:24.918371 : Inferring Test_File_12\n",
      "2022-08-24 14:19:29.641204 : Inferring Test_File_13\n",
      "2022-08-24 14:19:34.306264 : Inferring Test_File_14\n",
      "2022-08-24 14:19:39.226580 : Inferring Test_File_15\n",
      "2022-08-24 14:19:44.010206 : Inferring Test_File_16\n",
      "2022-08-24 14:19:48.542276 : Inferring Test_File_17\n",
      "2022-08-24 14:19:53.236500 : Inferring Test_File_18\n",
      "2022-08-24 14:19:57.942190 : Inferring Test_File_19\n",
      "2022-08-24 14:20:02.589198 : Inferring Test_File_20\n",
      "Submission file shape is (924621, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038be0571bd6b3776cb8512731968f4de302c811030124...</td>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...</td>\n",
       "      <td>0.103047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03a1d125bdd776000bf0b28238d0bea240ad581d332e70...</td>\n",
       "      <td>0.129862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...</td>\n",
       "      <td>0.315075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  prediction\n",
       "0  038be0571bd6b3776cb8512731968f4de302c811030124...    0.003534\n",
       "1  0074a0233ef766b52884608cc8cf9098f59d885b5d59fc...    0.000186\n",
       "2  060b8b7f30f795a0e93995d45b29461ffa6ece0eeb5c3d...    0.103047\n",
       "3  03a1d125bdd776000bf0b28238d0bea240ad581d332e70...    0.129862\n",
       "4  0290f245dd35ba899af52316ccc62b2627e7ae18cd76a2...    0.315075"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INFER_TEST_DATA = 1\n",
    "\n",
    "def infer_test_data():\n",
    "    # INFER TEST DATA\n",
    "    start = 0; end = 0\n",
    "    sub = pd.read_csv(FILENAME_SAMPLE_SUBMISSION_CSV)\n",
    "    \n",
    "    # REARANGE SUB ROWS TO MATCH PROCESSED TEST FILES\n",
    "    sub['hash'] = sub['customer_ID'].str[-16:].apply(lambda x:int(x,16)).astype('int64')\n",
    "    test_hash_index = np.load(f'{FILENAME_TEST_HASH_DATA}.npy')\n",
    "    sub = sub.set_index('hash').loc[test_hash_index].reset_index(drop=True)\n",
    "    \n",
    "    for k in range(NUM_TEST_FILES):\n",
    "        # BUILD MODEL\n",
    "        K.clear_session()\n",
    "        model = build_gru_model()\n",
    "        \n",
    "        # LOAD TEST DATA\n",
    "        utils.pt(f'Inferring Test_File_{k+1}')\n",
    "        X_test = np.load(f'{PATH_TO_PROCESSED2_DATA}test_data_{k+1}.npy')\n",
    "        end = start + X_test.shape[0]\n",
    "\n",
    "        # INFER 5 FOLD MODELS\n",
    "        model.load_weights(f'{PATH_TO_MODEL}gru_fold_0.h5')\n",
    "        p = model.predict(X_test, batch_size=512, verbose=0).flatten() \n",
    "        for j in range(1,5):\n",
    "            model.load_weights(f'{PATH_TO_MODEL}gru_fold_{j}.h5')\n",
    "            p += model.predict(X_test, batch_size=512, verbose=0).flatten()\n",
    "        p /= 5.0\n",
    "\n",
    "        # SAVE TEST PREDICTIONS\n",
    "        sub.loc[start:end-1,'prediction'] = p\n",
    "        start = end\n",
    "        \n",
    "        # CLEAN MEMORY\n",
    "        utils.gc_l([ model, X_test, p])\n",
    "     \n",
    "    sub.to_csv(FILENAME_GRU_SUBMISSION,index=False)\n",
    "    print('Submission file shape is', sub.shape )\n",
    "    display( sub.head() )\n",
    "\n",
    "    \n",
    "if INFER_TEST_DATA:\n",
    "    infer_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a871bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
